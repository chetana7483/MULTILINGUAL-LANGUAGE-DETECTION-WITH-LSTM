Multilingual Language Detection Using LSTM

A deep learningâ€“based language identification system that classifies text into English, Kannada, Hindi, or Tamil using a Character-Level Long Short-Term Memory (LSTM) neural network.
This project demonstrates how sequential deep learning models can effectively detect languages by analyzing character patterns instead of relying on word-level features.

ğŸš€ Project Overview

Multilingual language detection is an essential component in modern NLP applications such as:

Chatbots

Social media analytics

Translation engines

Information retrieval systems

Content moderation

Traditional rule-based or word-based methods often fail for short texts, informal writing, or mixed scripts.
This project solves that using an LSTM model, which learns script- and sequence-based patterns at the character level.

ğŸ“Œ Features

âœ” Character-level tokenizer for script-based language representation
âœ” Custom multilingual dataset (Wikipedia-sourced + augmented English data)
âœ” LSTM neural network built using TensorFlow/Keras
âœ” High accuracy on short and noisy text inputs
âœ” Visualizations: accuracy & loss curves + confusion matrix
âœ” Supports prediction for real-time text input

ğŸ“ Dataset

The dataset is built using:

Wikipedia sentences in English, Kannada, Hindi, and Tamil

Additional English samples for class balancing

Cleaned, tokenized, padded text sequences

Each row contains:

text	language
"à¤­à¤¾à¤°à¤¤ à¤à¤• à¤µà¤¿à¤¶à¤¾à¤² à¤¦à¥‡à¤¶ à¤¹à¥ˆà¥¤"	Hindi
"Welcome to the world of AI."	English
ğŸ§  Model Architecture

The LSTM model includes:

Embedding Layer â€“ character-level vector representation

LSTM Layer (128 units) â€“ learns sequential dependencies

Dropout Layer (0.3) â€“ prevents overfitting

Dense Layer â€“ classification using Softmax

Loss: categorical_crossentropy
Optimizer: Adam
Metrics: Accuracy

ğŸ”§ Tech Stack

Python

TensorFlow / Keras

NumPy, Pandas

Scikit-learn

Matplotlib

Jupyter Notebook / Google Colab

âš™ï¸ How to Run the Project
1. Clone the repository
git clone https://github.com/your-username/multilingual-language-detection-lstm.git
cd multilingual-language-detection-lstm

2. Install dependencies
pip install -r requirements.txt

3. Run the training script
python train_model.py

4. Test the model
python predict.py

ğŸ“Š Results

The model is evaluated using:

Accuracy

Precision, Recall, F1-score

Confusion Matrix

Training/Validation Accuracy graph

Training/Validation Loss graph

The LSTM model shows strong performance in distinguishing between four languages even for short sequences.



ğŸ“ Sample Usage
from language_identifier import predict_language

text = "à²¨à²¿à³•à²µà³ à²¹à³‡à²—à²¿à²¦à³à²¦à²¿à³•à²°à²¿?"
lang = predict_language(text)
print("Predicted Language:", lang)


Project Structure:
MULTILINGUAL/
â”‚
â”œâ”€â”€ app.py                         # (Optional) Script for running the prediction interface or API
â”œâ”€â”€ ex.py                          # Dataset collection script (Wikipedia scraping)
â”œâ”€â”€ language_identifier.py          # Main script to load model & predict language
â”œâ”€â”€ tempCodeRunnerFile.py           # Temporary VS Code runner file (auto-generated)
â”‚
â”œâ”€â”€ extra_english.csv               # Additional English dataset used for augmentation
â”œâ”€â”€ mini_multilingual.csv           # Base multilingual dataset from Wikipedia
â”œâ”€â”€ mini_multilingual_aug.csv       # Combined + augmented dataset
â”œâ”€â”€ input.txt                       # Sample input text file for testing
â”‚
â”œâ”€â”€ language_model.h5               # Trained LSTM language detection model
â”œâ”€â”€ tokenizer.pkl                   # Saved tokenizer (character-level)
â”œâ”€â”€ label_encoder.pkl               # Saved label encoder (maps classes to indices)
â”‚
â””â”€â”€ README.md (recommended) 

Output:

Predicted Language: Kannada

ğŸ“Œ Future Enhancements

Add more regional + global languages

Use GRU or Transformer-based architecture

Deploy as a REST API or Streamlit web app

Create a mobile-compatible prediction interface
